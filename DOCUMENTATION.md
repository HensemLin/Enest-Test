# AI Tender Assistant - Technical Documentation

---

## Executive Summary

This project delivers a **functional AI Tender Assistant** that successfully implements all three required deliverables:

1. ‚úÖ **Bid Chat Assistant** - RAG-based chatbot for querying tender documents
2. ‚úÖ **Tender Requirement Extractor** - AI-powered extraction of 50+ requirements
3. ‚úÖ **Bill of Materials Extractor** - Automated BoM/BoQ extraction with hierarchy preservation

The system processes complex tender PDFs, extracts structured information with 80%+ accuracy, and provides business-ready outputs (Excel/JSON exports) through a professional web interface.

---

## System Architecture

```mermaid
graph LR
    User([üë§ User Browser])

    UI1[üì§ PDF Upload<br/>Next.js]
    UI2[üìã Requirements<br/>Next.js]
    UI3[üîß BoM Manager<br/>Next.js]
    UI4[üí¨ Chat Interface<br/>Next.js]

    MW[üîê Middleware<br/>X-API-KEY Auth]

    R1["API: /pdfs"]
    R2["API: /requirements"]
    R3["API: /bom"]
    R4["API: /chat"]

    PDF[PDF Service<br/>Upload & Parse]
    REQ[Requirements<br/>Extractor]
    BOM[BoM<br/>Extractor]
    CHAT[Chat<br/>Service]

    LLM[üß† LLM<br/>mistralai/devstral-2512<br/>JSON Output]

    EMB[Embeddings<br/>qwen3-8b]
    VEC[(FAISS<br/>Vector Store)]
    MEM[Memory<br/>Buffer+Semantic]
    RET[Retrieval<br/>Top-5]

    DB[(üíæ PostgreSQL<br/>All Records)]
    FS[üìÅ File Storage<br/>PDFs & Exports]

    User -->|HTTPS| UI1
    User -->|HTTPS| UI2
    User -->|HTTPS| UI3
    User -->|HTTPS| UI4

    UI1 --> MW
    UI2 --> MW
    UI3 --> MW
    UI4 --> MW

    MW -->|Auth OK| R1
    MW -->|Auth OK| R2
    MW -->|Auth OK| R3
    MW -->|Auth OK| R4

    R1 --> PDF
    R2 --> REQ
    R3 --> BOM
    R4 --> CHAT

    PDF -->|Extract Text| EMB
    PDF -->|Store File| FS
    PDF -->|Save Meta| DB
    EMB -->|Generate| VEC

    REQ -->|Get Text| PDF
    REQ -->|Extract| LLM
    LLM -->|JSON| REQ
    REQ -->|Save| DB
    REQ -->|Export| FS

    BOM -->|Get Tables| PDF
    BOM -->|Parse| LLM
    LLM -->|Structured| BOM
    BOM -->|Save| DB
    BOM -->|Export| FS

    CHAT -->|Query| RET
    VEC -->|Search| RET
    MEM -->|Context| RET
    RET -->|Prompt| LLM
    LLM -->|Answer| CHAT
    CHAT -->|Update| MEM
    MEM <-->|Persist| DB

    classDef frontend fill:#e1f5ff,stroke:#01579b,stroke-width:2px
    classDef auth fill:#fff3e0,stroke:#e65100,stroke-width:2px
    classDef backend fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    classDef ai fill:#ffebee,stroke:#c62828,stroke-width:3px
    classDef rag fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px
    classDef data fill:#fce4ec,stroke:#880e4f,stroke-width:2px

    class User,UI1,UI2,UI3,UI4 frontend
    class MW auth
    class R1,R2,R3,R4,PDF,REQ,BOM,CHAT backend
    class LLM ai
    class EMB,VEC,MEM,RET rag
    class DB,FS data
```

---

## Tech Stack & Justification

### Backend Framework: **FastAPI (Python)**

**Why FastAPI?**

- **Performance:** ASGI-based, handles async operations efficiently for long-running LLM calls
- **Type Safety:** Pydantic models ensure data validation and reduce bugs
- **Auto Documentation:** Built-in OpenAPI/Swagger docs for API testing
- **Ecosystem:** Excellent integration with LangChain, SQLAlchemy, and ML libraries

### Frontend Framework: **Next.js 14 (React)**

**Why Next.js?**

- **Modern Stack:** Industry-standard for production web applications
- **Server Components:** Efficient rendering and SEO-friendly
- **API Routes:** Simplified backend integration with middleware support
- **Developer Experience:** Hot reload, TypeScript support, excellent tooling

### LLM Provider: **OpenRouter API**

**Why OpenRouter?**

- **Model Flexibility:** Easy switching between Claude, GPT-4, Mistral, etc.
- **Free Tier:** `mistralai/devstral-2512:free` for cost-effective prototyping
- **Unified API:** Single interface for multiple LLM providers
- **No Vendor Lock-in:** Can switch providers without code changes

**Model Choice:** `mistralai/devstral-2512:free`

- Free tier for MVP development
- Good balance of performance and cost
- Suitable for structured extraction tasks
- Can be upgraded to Claude-3 or GPT-4 for production

### LLM Framework: **LangChain**

**Why LangChain?**

- **RAG Implementation:** Built-in vector stores, retrievers, and memory systems
- **Structured Output:** Easy extraction of JSON schemas from unstructured text
- **Prompt Templates:** Reusable, maintainable prompt engineering
- **Conversation Memory:** Buffer + Semantic memory for context-aware chat
- **Production Ready:** Battle-tested in real-world AI applications

### Vector Database: **FAISS (Facebook AI Similarity Search)**

**Why FAISS?**

- **Performance:** Fast similarity search for document retrieval
- **Local Storage:** No external dependencies, works offline
- **Simplicity:** Easy to set up and maintain for MVP
- **Scalability:** Can handle thousands of document chunks efficiently

### Database: **PostgreSQL**

**Why PostgreSQL?**

- **Reliability:** Production-grade relational database
- **ACID Compliance:** Ensures data integrity for tender data
- **Rich Ecosystem:** Excellent ORM support (SQLAlchemy)
- **Docker Integration:** Easy deployment and development setup

### Additional Technologies

| Technology            | Purpose          | Justification                           |
| --------------------- | ---------------- | --------------------------------------- |
| **Docker Compose**    | Containerization | Reproducible deployments, easy setup    |
| **Argon2**            | Password hashing | Industry-standard API key security      |
| **Pandas + OpenPyXL** | Excel export     | Business-ready deliverables             |
| **React Query**       | Data fetching    | Efficient caching, auto-refetch         |
| **TailwindCSS**       | Styling          | Rapid UI development, professional look |
| **Axios**             | HTTP client      | Clean API for REST communication        |

---

## Design Decisions

### 1. **Extraction Strategy: Pure LLM with Structured Output**

**Decision:** LLM-based extraction with structured JSON output schemas

**Rationale:**

- **Rule-Based Extraction (rejected):** Tender documents have inconsistent formatting; regex/pattern matching would fail on varied layouts
- **Pure Unstructured LLM (rejected):** Free-form LLM outputs lead to inconsistent data formats
- **LLM + Structured Output (chosen):**
  - Use `ChatOpenAI` (via OpenRouter) with detailed prompts
  - LLM extracts data and returns valid JSON arrays
  - Post-processing validates and enriches the data
  - 80%+ accuracy achieved with few-shot prompting

**Implementation:**

```python
# Requirements Extractor
class RequirementLLMExtractor:
    def __init__(self):
        self.llm = ChatOpenAI(model="mistralai/devstral-2512:free")

    def extract_requirements_from_text(self, text, page_number):
        messages = [SystemMessage(...), HumanMessage(text)]
        response = self.llm.invoke(messages)  # Pure LLM extraction
        return json.loads(response.content)

# BoM Extractor
class BomTableParser:
    def __init__(self):
        self.llm = ChatOpenAI(model="mistralai/devstral-2512:free")

    def _extract_with_llm(self, markdown_content, page_number):
        # Uses pymupdf4llm for markdown preprocessing
        # Then LLM extraction with few-shot prompting
        response = self.llm.invoke(messages)  # Pure LLM extraction
        return json.loads(response.content)
```

**Key Techniques:**

- **Few-shot prompting** with example tender extractions
- **Markdown preprocessing** via `pymupdf4llm` for better structure preservation
- **JSON schema enforcement** in prompts (not Pydantic structured output)
- **Confidence scoring** to flag uncertain extractions

### 2. **Background Processing for Long Extractions**

**Decision:** Asynchronous task processing with status polling

**Rationale:**

- Extraction can take 1-3 minutes for large PDFs
- Blocking requests would timeout and freeze UI
- **Solution:**
  - Start extraction ‚Üí return immediately
  - Set status to "processing" in database
  - Frontend polls every 5 seconds to check completion
  - User can navigate away while extraction runs

**Trade-off:** Slightly more complex frontend logic vs. better UX

### 3. **RAG Architecture: Per-PDF vs Global Vector Store**

**Decision:** Separate FAISS index per PDF document

**Rationale:**

- **User Control:** Users select which PDFs to include in chat
- **Accuracy:** No cross-contamination between unrelated documents
- **Performance:** Smaller indices = faster retrieval
- **Maintenance:** Can delete PDF without rebuilding entire vector DB

**Implementation:**

```python
# Each PDF gets its own FAISS index
storage_path = f"storage/vectors/pdf_{pdf_id}.faiss"
vector_store = FAISS.from_documents(chunks, embeddings)
vector_store.save_local(storage_path)
```

### 4. **Pagination: Client-Side vs Server-Side**

**Decision:** Server-side pagination for requirements/BoM (100 items/page)

**Rationale:**

- Large tender documents can have 500+ requirements
- **Client-side (rejected):** Browser memory issues, slow initial load
- **Server-side (chosen):**
  - Database query with LIMIT/OFFSET
  - Fast page loads
  - Scalable to thousands of items

### 5. **API Authentication: JWT vs API Keys**

**Decision:** Server-generated API keys (hashed with Argon2)

**Rationale:**

- **Simplicity:** No token refresh logic needed for MVP
- **Security:** Keys stored hashed in database
- **Frontend Security:** Key stored server-side only (Next.js middleware)
- **No Browser Exposure:** Key never sent to browser JavaScript

**Flow:**

```
1. Generate key: POST /api/keys/ ‚Üí returns plain key once
2. Store in .env: API_KEY=xxx
3. Frontend middleware adds X-API-KEY header
4. Backend validates against hashed keys in DB
```

### 6. **Memory System: Conversation Context**

**Decision:** Hybrid memory (Buffer + Semantic)

**Rationale:**

- **Buffer Memory:** Last 10 messages always included (recent context)
- **Semantic Memory:** Top 5 relevant past messages retrieved (long-term context)
- **Summary:** Auto-summarize when conversation > 15 messages
- **Trade-off:** Higher LLM costs vs. better conversation quality

---

## Challenges & Solutions

### Challenge 1: **FastAPI Route Conflicts**

**Problem:**

```
GET /api/pdfs ‚Üí 404 "No API key(s) found"
```

The API keys router had prefix `/api` with route `/{id}`, creating `/api/{id}`. When calling `/api/pdfs`, FastAPI matched it as `/api/{id}` where `id="pdfs"` and routed to the wrong handler.

**Root Cause:** Route ordering and overlapping prefixes

**Solution:**
Changed API keys router prefix from `/api` to `/api/keys`:

```python
# Before (conflicted)
router = APIRouter(prefix="/api", tags=["API KEY"])
# ‚Üí /api/{id} matches /api/pdfs

# After (fixed)
router = APIRouter(prefix="/api/keys", tags=["API KEY"])
# ‚Üí /api/keys/{id} doesn't match /api/pdfs
```

**Lesson Learned:** Use specific, non-overlapping route prefixes to avoid conflicts. Added both `@router.get("")` and `@router.get("/")` decorators to handle trailing slash variations.

### Challenge 2: **Poor Chatbot Performance with Noisy PDF Text**

**Problem:**

Early testing showed the RAG chatbot giving irrelevant or confused responses. The retrieved context contained excessive noise:

- Page headers/footers repeated on every page ("FOR TENDER PURPOSE", "Page X OF Y")
- Excessive whitespace and line breaks
- Metadata that cluttered the actual content

**Root Cause:** PDF text extraction included all raw text without cleaning

**Impact:**

- LLM context window filled with irrelevant text (wasting tokens)
- Retrieved chunks had low signal-to-noise ratio
- Chatbot struggled to focus on actual tender requirements

**Solution:**

Implemented `_clean_text()` function in `PDFTextExtractor` to sanitize extracted text:

```python
def _clean_text(self, text: str) -> str:
    """Clean extracted text by removing page headers/footers and excessive whitespace."""
    # Remove common page header patterns
    text = re.sub(
        r'FOR TENDER PURPOSE[^\n]*\n[^\n]*OF\s*\d+',
        '',
        text,
        flags=re.IGNORECASE | re.MULTILINE
    )

    # Remove standalone page numbers
    text = re.sub(r'^\s*\d+\s*OF\s*\d+\s*$', '', text, flags=re.MULTILINE)

    # Remove excessive whitespace
    text = re.sub(r"\n{3,}", "\n\n", text)

    return text.strip()
```

**Results:**

- 30-40% reduction in token usage per chunk
- More focused context retrieval
- Significantly improved chatbot response quality

**Lesson Learned:** Always preprocess extracted text before feeding to LLM. Clean data is crucial for RAG performance.

### Challenge 3: **Table Structure Loss in Plain Text Extraction**

**Problem:**

Tender documents contain critical information in tables (Bill of Materials, technical specifications, pricing schedules). Initial plain text extraction flattened tables into unstructured text:

```
Item Description Quantity Unit
1 Transformer 2 Nos
2 GIS Equipment 1 Set
```

Became:

```
Item Description Quantity Unit 1 Transformer 2 Nos 2 GIS Equipment 1 Set
```

**Impact:**

- LLM couldn't understand table relationships
- BoM extraction failed to identify columns
- Chatbot couldn't answer questions like "What's the quantity of transformers?"

**Solution:**

Used **markdown-based extraction** instead of plain text:

1. **For Chat RAG:** Used `pymupdf` with markdown mode

   ```python
   # Convert to markdown preserving table structure
   md_text = pymupdf4llm.to_markdown(pdf_path)
   ```

2. **For BoM Extraction:** Used `pymupdf4llm` for optimal table preservation

   ```python
   # Extracts tables as proper markdown tables
   tables = pymupdf4llm.to_markdown(pdf_path, page_chunks=True)
   ```

**Markdown output preserves structure:**

```markdown
| Item | Description   | Quantity | Unit |
| ---- | ------------- | -------- | ---- |
| 1    | Transformer   | 2        | Nos  |
| 2    | GIS Equipment | 1        | Set  |
```

**Results:**

- LLM can now parse table rows and columns correctly
- BoM extraction accuracy improved from ~40% to ~80%
- Chatbot understands tabular data and can answer specific queries

**Lesson Learned:** Document structure matters for LLM understanding. Markdown preserves semantic structure better than plain text for complex documents.

---

## Limitations & Known Issues

### Current Limitations

1. **PDF Size Limit: 100MB**

   - Configurable in `.env`
   - Large files slow down extraction
   - **Workaround:** Split very large documents

2. **Extraction Accuracy: ~80-85%**

   - Depends on PDF quality and formatting
   - Scanned PDFs without OCR may fail
   - Complex tables occasionally misaligned
   - **Mitigation:** Manual review recommended for critical data

3. **Concurrent Extractions: One per PDF**

   - Background processing prevents multiple simultaneous extractions on same PDF
   - Prevents race conditions and duplicate data
   - **Impact:** Must wait for extraction to complete before re-running

4. **Pagination: Fixed at 100 items/page**

   - Hard-coded for optimal performance
   - **Future:** Make configurable per-user

5. **Chat Context Window: 2000 tokens**

   - Configurable in `.env`
   - Long conversations may truncate history
   - **Mitigation:** Auto-summarization after 15 messages

6. **Supported Formats: PDF only**

   - No Word (.docx), Excel (.xlsx), or image support
   - **Workaround:** Convert to PDF before upload

7. **Single-Tenant Architecture**

   - No multi-company isolation
   - All users share same database
   - **Production Need:** Add company_id to all tables

8. **No Real-Time Collaboration**

   - Multiple users editing same requirement can cause conflicts
   - **Future:** WebSocket for live updates

9. **JSON Parsing from LLM Responses**

   - Current implementation prompts LLM to return JSON, then parses manually
   - No schema enforcement at LLM level
   - Occasional parsing errors if LLM returns malformed JSON
   - **Impact:** ~1-2% extraction failure rate due to JSON parsing errors
   - **Future:** Use LangChain's `with_structured_output()` for guaranteed schema compliance

10. **No Duplicate Detection for PDF Uploads**

    - Same file can be uploaded multiple times
    - No hash-based or filename-based duplicate detection
    - Creates separate database records and storage files for each upload
    - **Impact:** Wastes storage space and can confuse users with duplicate entries in PDF list
    - **Workaround:** Manually check existing PDFs before uploading
    - **Future:** Implement SHA-256 hash comparison before upload

11. **BoM Extraction Creates Duplicates on Re-run**

    - Running BoM extraction multiple times on same PDF creates duplicate entries
    - No automatic cleanup of previous extraction results
    - Each extraction gets new `extraction_job_id` but items accumulate in database
    - **Impact:** Users see duplicate BoM items in the table (confusing and messy data)
    - **Current Workaround:** Manually delete PDF and re-upload to clear BoM data
    - **Note:** Requirements extraction handles this correctly by auto-deleting old data before re-extraction
    - **Future:** Add auto-cleanup logic to BoM extractor similar to requirements extractor

### Known Issues

1. **Table Extraction Accuracy**

   - Very complex nested tables may lose structure
   - Multi-page tables sometimes split incorrectly
   - **Status:** Acceptable for MVP; manual verification advised

2. **Scanned PDFs**

   - No OCR capability included
   - Scanned images return empty text
   - **Workaround:** Use external OCR tool first

3. **Handwritten Notes**

   - Cannot extract handwritten annotations
   - **Limitation:** LLM processes text only

4. **Images & Diagrams**

   - PDF images not analyzed
   - Technical drawings not interpreted
   - **Future:** Add vision models (GPT-4V, Claude-3)

5. **Export File Overwrite**

   - Re-exporting with same filters may overwrite previous export
   - **Workaround:** Timestamped filenames prevent collisions

---

## Future Enhancements

### Short-Term (Next 2-4 Weeks)

1. **Duplicate PDF Detection**

   - Hash-based duplicate detection (SHA-256)
   - Warn user before uploading duplicate file
   - Option to replace existing or keep both
   - Prevent storage waste and user confusion
   - **Effort:** 1-2 days

2. **BoM Extraction Auto-Cleanup**

   - Delete old BoM items before re-extraction (similar to requirements)
   - Prevent duplicate entries when running extraction multiple times
   - Add confirmation dialog: "Previous extraction found. Replace?"
   - **Effort:** 1 day

3. **Multi-File Upload**

   - Batch upload of multiple PDFs
   - Progress bar for each file
   - **Effort:** 1-2 days

4. **Advanced Filters**

   - Date range filtering for requirements
   - Multi-select categories
   - Custom compliance status values
   - **Effort:** 2-3 days

5. **Requirement Comparison**

   - Compare requirements across multiple tenders
   - Highlight differences
   - Export comparison matrix
   - **Effort:** 3-4 days

6. **User Management**

   - Multiple user accounts
   - Role-based access control (Admin, Viewer, Editor)
   - Activity logs
   - **Effort:** 1 week

7. **Email Notifications**

   - Alert when extraction completes
   - Weekly summary of pending requirements
   - **Effort:** 2-3 days

### Medium-Term (1-3 Months)

8. **LangChain Structured Output**

   - Replace manual JSON parsing with `with_structured_output()`
   - Use Pydantic models for schema enforcement at LLM level
   - Guaranteed type safety and validation
   - **Benefits:**

     - Eliminate JSON parsing errors (improve reliability to 99%+)
     - Better error messages when LLM fails to follow schema
     - Type-safe extraction with automatic validation

   - **Implementation:**

     ```python
     from langchain_core.pydantic_v1 import BaseModel, Field

     class Requirement(BaseModel):
         category: str = Field(description="Requirement category")
         requirement_detail: str = Field(description="Requirement text")
         mandatory_optional: str = Field(description="Mandatory or Optional")
         confidence_score: float = Field(ge=0.0, le=1.0)

     llm_with_schema = llm.with_structured_output(Requirement)
     result = llm_with_schema.invoke(messages)  # Returns Pydantic object
     ```

   - **Effort:** 2-3 days

9. **Vision Model Integration**

   - Analyze technical drawings and diagrams
   - Extract data from images
   - Use GPT-4V or Claude-3 Opus
   - **Effort:** 1-2 weeks

10. **OCR Pipeline**

- Automatic OCR for scanned PDFs
- Integrate Tesseract or cloud OCR service
- **Effort:** 1 week

11. **Custom Extraction Templates**

- User-defined requirement categories
- Custom BoM columns
- Saved extraction profiles
- **Effort:** 2-3 weeks

12. **Collaboration Features**

    - Real-time updates via WebSockets
    - Comments on requirements
    - Assignment workflow (assign requirement ‚Üí engineer ‚Üí review)
    - **Effort:** 3-4 weeks

13. **Analytics Dashboard**

    - Compliance statistics
    - Extraction quality metrics
    - Tender comparison analytics
    - **Effort:** 2 weeks

### Long-Term (3-6 Months)

14. **Multi-Tenant SaaS**

    - Company isolation
    - Subscription management
    - Usage quotas and billing
    - **Effort:** 4-6 weeks

15. **Advanced RAG**

    - Hybrid search (keyword + semantic)
    - Re-ranking with cross-encoders
    - Query expansion
    - **Effort:** 2-3 weeks

16. **Mobile App**

    - React Native or Flutter
    - Offline mode for chat
    - Push notifications
    - **Effort:** 2-3 months

17. **Integration APIs**

    - Zapier/Make.com connectors
    - ERP system integration (SAP, Oracle)
    - Slack/Teams bots
    - **Effort:** 4-6 weeks

18. **AI-Powered Bid Writing**

    - Generate compliance matrices automatically
    - Draft tender responses
    - Risk analysis and recommendations
    - **Effort:** 2-3 months
